# Video Recording Checklist

## üé• Video 1: Kafka Data Streaming Demo

### **What to Show (2-3 minutes):**

#### **1. Setup Phase (30 seconds)**
- [ ] Show Redpanda Console: `http://localhost:8080`
- [ ] Show the "orders" topic being created
- [ ] Show consumer and producer windows starting

#### **2. Data Flow (1-2 minutes)**
- [ ] **Producer Window**: Show events being generated
  ```
  Event: Order 123 -> CREATED
  Event: Order 456 -> PAID
  Event: Order 789 -> SHIPPED
  ```
- [ ] **Redpanda Console**: Show messages flowing through the topic
- [ ] **Consumer Window**: Show events being consumed and sent to Snowflake
  ```
  Consumed event: Order 123 -> CREATED
  Writing to Snowflake...
  ```

#### **3. Snowflake Validation (30 seconds)**
- [ ] Open Snowflake console
- [ ] Show `RETAIL.RAW.EVENTS` table with new data
- [ ] Run count query: `SELECT COUNT(*) FROM RETAIL.RAW.EVENTS;`

---

## üé• Video 2: Snowflake Automation Process Demo

### **What to Show (3-4 minutes):**

#### **1. Automation Setup (30 seconds)**
- [ ] Show Stream: `RETAIL.RAW.EVENTS_STRM`
- [ ] Show Task: `TASK_AUTO_UPDATE_ORDER_STATUS`
- [ ] Show Task is "STARTED" (not suspended)

#### **2. Event Generation (1-2 minutes)**
- [ ] Run: `python demo_automation.py`
- [ ] Show events being generated slowly
- [ ] Show `RETAIL.RAW.EVENTS` table growing

#### **3. Automation in Action (1-2 minutes)**
- [ ] Show Stream data: `SELECT * FROM RETAIL.RAW.EVENTS_STRM;`
- [ ] Wait for task to run (every 2 minutes)
- [ ] Show Task History: 
  ```sql
  SELECT * FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY('RETAIL.DWH.TASK_AUTO_UPDATE_ORDER_STATUS'));
  ```
- [ ] Show DWH table updates: `SELECT * FROM RETAIL.DWH.ORDER_STATUS;`

#### **4. Validation (30 seconds)**
- [ ] Run validation queries from `stream_task_tvalidation.sql`
- [ ] Show data consistency between RAW and DWH

---

## üé• Video 3: Monitoring Dashboard Demo

### **What to Show (2-3 minutes):**

#### **1. Dashboard Launch (30 seconds)**
- [ ] Run: `streamlit run monitoring/app.py`
- [ ] Show dashboard opening at `http://localhost:8501`

#### **2. Real-time Monitoring (1-2 minutes)**
- [ ] Show order status distribution chart
- [ ] Show event counts
- [ ] Show real-time updates as events flow

#### **3. Interactive Features (30 seconds)**
- [ ] Show filtering options
- [ ] Show refresh functionality
- [ ] Show different views/metrics

---

## üìã Key Things to Check During Recording

### **Always Verify:**

#### **1. Services are Running**
- [ ] Docker containers: `docker compose ps`
- [ ] Redpanda: `http://localhost:8080`
- [ ] Consumer window: Shows "Consuming events..."
- [ ] Producer window: Shows "Producing events..."

#### **2. Data is Flowing**
- [ ] Events appear in Redpanda Console
- [ ] Consumer shows "Writing to Snowflake..."
- [ ] Snowflake tables have new data
- [ ] No error messages in any window

#### **3. Automation is Working**
- [ ] Stream shows new records: `SELECT * FROM RETAIL.RAW.EVENTS_STRM;`
- [ ] Task shows "STARTED" status
- [ ] Task history shows successful runs
- [ ] DWH table gets updated

#### **4. Data Quality**
- [ ] Event counts match between RAW and DWH
- [ ] No duplicate records
- [ ] Status transitions make sense
- [ ] Timestamps are recent

---

## üö® Common Issues to Watch For

### **If Something Goes Wrong:**

#### **Redpanda Issues:**
- [ ] Check: `docker compose logs redpanda`
- [ ] Restart: `docker compose restart redpanda`

#### **Consumer Issues:**
- [ ] Check Snowflake credentials in `.env`
- [ ] Verify database exists: `RETAIL.RAW.EVENTS`
- [ ] Check network connectivity

#### **Producer Issues:**
- [ ] Check Redpanda is running
- [ ] Verify topic exists: `orders`
- [ ] Check Kafka connection

#### **Automation Issues:**
- [ ] Check task is not suspended: `SHOW TASKS`
- [ ] Verify stream exists: `SHOW STREAMS`
- [ ] Check warehouse is running

---

## üìù Script for Your Video

### **Opening (10 seconds):**
"Hi, I'm going to demonstrate my ETL pipeline with Kafka and Snowflake automation. This shows real-time data streaming and automated processing."

### **For Kafka Video:**
"First, I'll show the data streaming process. Events are generated by the producer, flow through Kafka, and are consumed into Snowflake."

### **For Automation Video:**
"Now I'll demonstrate the automation process. Snowflake streams detect changes and tasks automatically update the data warehouse."

### **For Monitoring Video:**
"Finally, here's the monitoring dashboard that shows real-time metrics and data quality."

### **Closing (10 seconds):**
"This completes the demonstration of the ETL pipeline with real-time streaming and automation."

---

## üéØ Success Criteria

### **Your video should show:**
- [ ] Data flowing from producer to consumer
- [ ] Events appearing in Snowflake
- [ ] Automation working (streams and tasks)
- [ ] Monitoring dashboard updating
- [ ] No errors or failures
- [ ] Clear explanation of what's happening

### **Technical validation:**
- [ ] Event counts increase over time
- [ ] DWH table gets updated automatically
- [ ] Stream shows new records
- [ ] Task runs successfully
- [ ] Dashboard shows real-time data

---

## üí° Pro Tips

1. **Prepare beforehand**: Test everything works before recording
2. **Keep it simple**: Don't show too many technical details
3. **Explain as you go**: Say what you're doing and why
4. **Show results**: Always end with validation queries
5. **Keep it short**: 2-4 minutes per video is perfect
6. **Clean interface**: Close unnecessary applications
7. **Good audio**: Speak clearly and not too fast

---

## üîß Quick Commands to Have Ready

```bash
# Check services
docker compose ps

# Check Redpanda
http://localhost:8080

# Check Snowflake
SELECT COUNT(*) FROM RETAIL.RAW.EVENTS;
SELECT COUNT(*) FROM RETAIL.DWH.ORDER_STATUS;

# Check automation
SHOW STREAMS IN SCHEMA RETAIL.RAW;
SHOW TASKS IN SCHEMA RETAIL.DWH;

# Check monitoring
streamlit run monitoring/app.py
```

Remember: The goal is to show that your pipeline works end-to-end with real-time data flow and automation!
